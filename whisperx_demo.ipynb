{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "device = \"cuda\"\n",
    "audio_file = \"001.mp3\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en (1.00) in first 30s of audio...\n",
      "                               segment label     speaker        start  \\\n",
      "0    [ 00:00:06.780 -->  00:00:10.189]     A  SPEAKER_00     6.780969   \n",
      "1    [ 00:00:10.392 -->  00:00:10.965]     B  SPEAKER_00    10.392219   \n",
      "2    [ 00:00:10.965 -->  00:00:11.286]     C  SPEAKER_03    10.965969   \n",
      "3    [ 00:00:12.484 -->  00:00:16.197]     D  SPEAKER_03    12.484719   \n",
      "4    [ 00:00:16.787 -->  00:00:18.374]     E  SPEAKER_03    16.787844   \n",
      "..                                 ...   ...         ...          ...   \n",
      "822  [ 00:55:18.330 -->  00:55:22.887]   AEQ  SPEAKER_03  3318.330969   \n",
      "823  [ 00:55:23.747 -->  00:55:24.979]   AER  SPEAKER_03  3323.747844   \n",
      "824  [ 00:55:27.713 -->  00:55:28.607]   AES  SPEAKER_03  3327.713469   \n",
      "825  [ 00:55:30.902 -->  00:55:31.274]   AET  SPEAKER_03  3330.902844   \n",
      "826  [ 00:55:36.167 -->  00:55:36.724]   AEU  SPEAKER_03  3336.167844   \n",
      "\n",
      "             end  intersection        union  \n",
      "0      10.189719  -3311.727281  3315.216031  \n",
      "1      10.965969  -3310.951031  3311.604781  \n",
      "2      11.286594  -3310.630406  3311.031031  \n",
      "3      16.197219  -3305.719781  3309.512281  \n",
      "4      18.374094  -3303.542906  3305.209156  \n",
      "..           ...           ...          ...  \n",
      "822  3322.887219      0.080000     4.556250  \n",
      "823  3324.979719     -1.750844     3.062719  \n",
      "824  3328.607844     -5.716469     6.690844  \n",
      "825  3331.274094     -8.905844     9.357094  \n",
      "826  3336.724719    -14.170844    14.807719  \n",
      "\n",
      "[827 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "# print(result[\"segments\"])\n",
    "\n",
    "model_a, metadata = whisperx.load_align_model(\n",
    "    language_code=result[\"language\"], device=device\n",
    ")\n",
    "result = whisperx.align(\n",
    "    result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False\n",
    ")\n",
    "\n",
    "# print(result[\"segments\"])\n",
    "\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=token, device=device)\n",
    "\n",
    "diarize_segments = diarize_model(audio)\n",
    "diarize_model(audio, min_speakers=2, max_speakers=2)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "print(diarize_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "segments = result[\"segments\"]\n",
    "\n",
    "f = open(\"transcription.txt\", \"x\", encoding=\"utf-8\")\n",
    "for segment in segments:\n",
    "    start = segment[\"start\"]\n",
    "    end = segment[\"end\"]\n",
    "    speaker = segment[\"speaker\"] if segment.get(\"speaker\") else \"UNKNOWN\"\n",
    "    text = segment[\"text\"]\n",
    "    line = (\n",
    "        f\"[{timedelta(seconds=start)} - {timedelta(seconds=end)}] {speaker} - {text}\\n\"\n",
    "    )\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
